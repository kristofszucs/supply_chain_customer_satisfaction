{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207787c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02672afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_soup_for_page(page_number):\n",
    "    \"\"\"We initialize a soup object that will contain all the tags that are on the main part of the Trustpilot website for the company Asurion : https://www.trustpilot.com/review/www.asurion.com\n",
    "        The website contains client ratings on multiple pages.\n",
    "        In the function we can choose for which page number we want to retrieve the data.\n",
    "    -----------------\n",
    "       Parameters\n",
    "    -----------------\n",
    "    page_number : insert an int value between 1 and the biggest existing page nuymber for the company \n",
    "\n",
    "    -----------------\n",
    "        Returns\n",
    "    -----------------\n",
    "    soup object containing the tags of the selected website\n",
    "    \"\"\"\n",
    "    \n",
    "    url = ''\n",
    "    if page_number == 1:\n",
    "        url = \"https://www.trustpilot.com/review/www.asurion.com\"\n",
    "    else:\n",
    "        url = f\"https://www.trustpilot.com/review/www.asurion.com?page={page_number}\"\n",
    "    page = urlopen(url)\n",
    "    soup = bs(page, \"html.parser\")\n",
    "    evaluations = soup.findAll('div', attrs = {'class' : \"styles_cardWrapper__LcCPA styles_show__HUXRb styles_reviewCard__9HxJJ\"})\n",
    "    return evaluations\n",
    "    \n",
    "def insert_row(df, my_row):\n",
    "    \"\"\"Insert a list in an existing DataFrame. The length of the list must be the same as the number of the columns in the DataFrame.\n",
    "    -----------------\n",
    "       Parameters\n",
    "    -----------------\n",
    "    df : the DataFrame in which we want to insert a new list as the last row of the DataFrame \n",
    "    my_row : the list we want to insert into the DataFrame\n",
    "    \n",
    "    -----------------\n",
    "        Returns\n",
    "    -----------------\n",
    "    None\n",
    "    \"\"\"\n",
    "    df.loc[len(df)] = my_row\n",
    "\n",
    "def get_specific_data_for_page(page_number):   \n",
    "    \"\"\"\n",
    "    -----------------\n",
    "       Parameters\n",
    "    -----------------\n",
    "    \n",
    "    -----------------\n",
    "        Returns\n",
    "    -----------------\n",
    "    \"\"\"\n",
    "        \n",
    "    df_evals = pd.DataFrame(columns = ['titre','nom','stars','localisation','nb_reviews','date','comment'])\n",
    "    evaluations = initialize_soup_for_page(page_number)\n",
    "    for e in evaluations:\n",
    "        titre_ = e.find('h2', {'class': 'typography_heading-s__f7029 typography_appearance-default__AAY17'})\n",
    "        titre = titre_.text if titre_ is not None else ''\n",
    "        \n",
    "        nom_ = e.find('span', {'class': 'typography_heading-xxs__QKBS8 typography_appearance-default__AAY17'})\n",
    "        nom = nom_.text if nom_ is not None else ''\n",
    "        \n",
    "        stars_ = e.find('img')['alt']\n",
    "        stars = stars_#.text if stars_ is not None else ''\n",
    "        \n",
    "        localisation_ = e.find('div', {'class': 'typography_body-m__xgxZ_ typography_appearance-subtle__8_H2l styles_detailsIcon__Fo_ua'}).find('span')\n",
    "        localisation = localisation_.text if localisation_ is not None else ''\n",
    "        \n",
    "        nb_reviews_ = e.find('span', {'class': 'typography_body-m__xgxZ_ typography_appearance-subtle__8_H2l'})\n",
    "        nb_reviews = nb_reviews_.text if nb_reviews_ is not None else ''\n",
    "        \n",
    "        date_ = e.find('time')\n",
    "        date = date_.text if date_ is not None else ''\n",
    "        \n",
    "        comment_ = e.find('p', {'class': 'typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn'})\n",
    "        comment = comment_.text if comment_ is not None else ''\n",
    "        \n",
    "        insert_row(df_evals, [titre,nom,stars,localisation,nb_reviews,date,comment])\n",
    "    return df_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8b0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_data_for_specific_pages(first_page=1,last_page=5000):\n",
    "    \"\"\" faut gerer le nb de page max automatiquement\n",
    "    -----------------\n",
    "       Parameters\n",
    "    -----------------\n",
    "    \n",
    "    -----------------\n",
    "        Returns\n",
    "    -----------------\n",
    "    \"\"\"\n",
    "    df_evals = pd.DataFrame(columns = ['titre','nom','stars','localisation','nb_reviews','date','comment'])\n",
    "    for n in range(first_page,last_page):\n",
    "        try :\n",
    "            df_evals = pd.concat([df_evals,get_specific_data_for_page(n)])\n",
    "            print(f\"Page {n} was succesfully scraped.\") if n%10 == 0 else None\n",
    "        except:\n",
    "            print(f\"For some reason data scraping couldn't be executed for page {n}.\")\n",
    "            print(\"Maybe the page we selected to scrape doesn't exist.\")\n",
    "            print(\"Or we reached the amount of data we could scrape within in a time period.\")\n",
    "            print(\"Let's wait a few minutes and start scraping again.\")\n",
    "            print(\"===\"*20)\n",
    "            print(\"Countdown 8 minutes :\")\n",
    "            print(\"===\"*20)\n",
    "            for i in range(480,0,-1):\n",
    "                time.sleep(1)\n",
    "                sys.stdout.write(str(i)+', ')\n",
    "            df_evals = pd.concat([df_evals, get_specific_data_for_page(n)])\n",
    "            print(f\"Page {n} was succesfully scraped.\")\n",
    "    return df_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef3cb3e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 10 was succesfully scraped.\n",
      "Page 20 was succesfully scraped.\n",
      "Page 30 was succesfully scraped.\n",
      "Page 40 was succesfully scraped.\n",
      "Page 50 was succesfully scraped.\n",
      "Page 60 was succesfully scraped.\n",
      "Page 70 was succesfully scraped.\n",
      "Page 80 was succesfully scraped.\n",
      "Page 90 was succesfully scraped.\n"
     ]
    }
   ],
   "source": [
    "df = get_specific_data_for_specific_pages(first_page = 1, last_page = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5a8b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titre</th>\n",
       "      <th>nom</th>\n",
       "      <th>stars</th>\n",
       "      <th>localisation</th>\n",
       "      <th>nb_reviews</th>\n",
       "      <th>date</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My screen shattered and I was leaving…</td>\n",
       "      <td>Connie G</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>17 hours ago</td>\n",
       "      <td>My screen shattered and I was leaving town, I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I Recommend Purchasing a Asurion Plan</td>\n",
       "      <td>Estelle F</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>US</td>\n",
       "      <td>8 reviews</td>\n",
       "      <td>18 hours ago</td>\n",
       "      <td>I make a good deal of purchases on Amazon.com ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I know sometimes that you think that…</td>\n",
       "      <td>Kimberly</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>12 hours ago</td>\n",
       "      <td>I know sometimes that you think that getting a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Glad a bought the warranty!!!</td>\n",
       "      <td>Melanie</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>15 hours ago</td>\n",
       "      <td>Christa live chatted with me in a quick and ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Asurion did me a great service!!!</td>\n",
       "      <td>William Gunter</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>US</td>\n",
       "      <td>3 reviews</td>\n",
       "      <td>17 hours ago</td>\n",
       "      <td>Everything was simple doing it all online, pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    titre             nom  \\\n",
       "0  My screen shattered and I was leaving…        Connie G   \n",
       "1   I Recommend Purchasing a Asurion Plan       Estelle F   \n",
       "2   I know sometimes that you think that…        Kimberly   \n",
       "3           Glad a bought the warranty!!!         Melanie   \n",
       "4       Asurion did me a great service!!!  William Gunter   \n",
       "\n",
       "                    stars localisation nb_reviews          date  \\\n",
       "0  Rated 5 out of 5 stars           US   1 review  17 hours ago   \n",
       "1  Rated 5 out of 5 stars           US  8 reviews  18 hours ago   \n",
       "2  Rated 5 out of 5 stars           US   1 review  12 hours ago   \n",
       "3  Rated 5 out of 5 stars           US   1 review  15 hours ago   \n",
       "4  Rated 5 out of 5 stars           US  3 reviews  17 hours ago   \n",
       "\n",
       "                                             comment  \n",
       "0  My screen shattered and I was leaving town, I ...  \n",
       "1  I make a good deal of purchases on Amazon.com ...  \n",
       "2  I know sometimes that you think that getting a...  \n",
       "3  Christa live chatted with me in a quick and ef...  \n",
       "4  Everything was simple doing it all online, pro...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.to_csv('asurion.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815e7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e2200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf365c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558c0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ebd049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f81b3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218dfdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15d585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6eeab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe3447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e7897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test getting data for page\n",
    "page_num = 250\n",
    "url = f\"https://www.trustpilot.com/review/www.asurion.com?page={page_num}\"\n",
    "page = urlopen(url)\n",
    "soup = bs(page, \"html.parser\")\n",
    "evaluations = soup.findAll('div', attrs = {'class' : \"styles_cardWrapper__LcCPA styles_show__HUXRb styles_reviewCard__9HxJJ\"})\n",
    "\n",
    "#print(evaluations)\n",
    "df_evals = pd.DataFrame(columns = ['titre','nom','stars','localisation','nb_reviews','date','comment'])\n",
    "for e in evaluations:\n",
    "        titre_ = e.find('h2', {'class': 'typography_heading-s__f7029 typography_appearance-default__AAY17'})\n",
    "        titre = titre_.text if titre_ is not None else ''\n",
    "        \n",
    "        nom_ = e.find('span', {'class': 'typography_heading-xxs__QKBS8 typography_appearance-default__AAY17'})\n",
    "        nom = nom_.text if nom_ is not None else ''\n",
    "        \n",
    "        stars_ = e.find('img')['alt']\n",
    "        stars = stars_#.text if stars_ is not None else ''\n",
    "        \n",
    "        localisation_ = e.find('div', {'class': 'typography_body-m__xgxZ_ typography_appearance-subtle__8_H2l styles_detailsIcon__Fo_ua'}).find('span')\n",
    "        localisation = localisation_.text if localisation_ is not None else ''\n",
    "        \n",
    "        nb_reviews_ = e.find('span', {'class': 'typography_body-m__xgxZ_ typography_appearance-subtle__8_H2l'})\n",
    "        nb_reviews = nb_reviews_.text if nb_reviews_ is not None else ''\n",
    "        \n",
    "        date_ = e.find('time')\n",
    "        date = date_.text if date_ is not None else ''\n",
    "        \n",
    "        comment_ = e.find('p', {'class': 'typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn'})\n",
    "        comment = comment_.text if comment_ is not None else ''\n",
    "        \n",
    "        insert_row(df_evals, [titre,nom,stars,localisation,nb_reviews,date,comment])\n",
    "        \n",
    "df_evals.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
